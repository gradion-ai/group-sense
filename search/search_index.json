{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Group Sense detects patterns in group chat message streams and transforms them into self-contained queries for downstream AI systems. While single-user AI assistants excel at responding to direct queries, they struggle with multi-party conversations where relevant information emerges from complex exchanges between multiple participants. Group Sense solves this by acting as an intelligent adapter that monitors group conversations, identifies meaningful patterns, and reformulates them into queries that existing AI assistants can process, enabling proactive and \"overhearing\" AI assistance without requiring the underlying assistant to understand group dynamics or multi-party dialogue structure.</p> <p>The library provides three core capabilities that make group chat AI assistance practical and flexible. First, it detects conversation patterns and transforms multi-party dialogue into self-contained queries that preserve essential context while removing conversational complexity. Second, engagement criteria are defined in natural language rather than code, allowing you to specify when and how the AI should participate using clear, human-readable rules. Third, Group Sense works as a non-invasive adapter for any existing single-user AI assistant or agent: no modification, retraining, or specialized models required. This architecture lets you add group chat capabilities to AI systems you already use, whether for collaborative decision-making, team coordination, or ambient assistance scenarios.</p>"},{"location":"#next-steps","title":"Next steps","text":"<ol> <li>Install the library and configure API keys</li> <li>Learn the core concepts and reasoner types</li> <li>Explore usage examples for different engagement patterns</li> <li>Integrate Group Sense into your application</li> </ol>"},{"location":"#llm-optimized-documentation","title":"LLM-optimized documentation","text":"<ul> <li>llms.txt</li> <li>llms-full.txt</li> </ul>"},{"location":"basics/","title":"Basics","text":"<p>Group Sense processes messages through reasoners that analyze incoming group chat messages and decide whether to ignore them or delegate them to an AI assistant. Each message has a sender (the user who wrote it) and content. When a reasoner decides to delegate, it generates a self-contained query suitable for a single-user AI assistant and optionally specifies which user should receive the response.</p> <p>Reasoners see the complete group chat context - every message from every user. The difference between reasoner types is how they maintain their internal reasoning state across messages.</p>"},{"location":"basics/#defaultgroupreasoner","title":"DefaultGroupReasoner","text":"<p><code>DefaultGroupReasoner</code> uses a single reasoner agent with shared reasoning state. All messages are processed through one conversation history, providing a unified perspective across all users.</p> <pre><code>from group_sense import Decision, DefaultGroupReasoner, Message\n\n\n# Load system prompt from file\nprompt_path = Path(\"examples\", \"prompts\", \"default\", \"fact_check.md\")\nsystem_prompt = prompt_path.read_text()\n\n# Create reasoner\nreasoner = DefaultGroupReasoner(system_prompt=system_prompt)\n\n# Process group chat messages\nlogger.info(\"Processing first batch of messages...\")\nresponse = await reasoner.process(\n    [\n        Message(content=\"The meeting is tomorrow at 2pm.\", sender=\"alice\"),\n        Message(content=\"Thanks for the reminder!\", sender=\"charlie\"),\n    ]\n)\nlogger.info(f\"Decision: {response.decision}\")\n# Decision: IGNORE (no contradiction detected)\n\nlogger.info(\"\\nProcessing message with contradiction...\")\nresponse = await reasoner.process(\n    [\n        Message(content=\"See you at 3pm tomorrow.\", sender=\"bob\"),\n    ]\n)\nlogger.info(f\"Decision: {response.decision}\")\nif response.decision == Decision.DELEGATE:\n    logger.info(f\"Query: {response.query}\")\n    logger.info(\"Group dialogue transformed into self-contained verification query\")\n</code></pre> <p>The reasoner maintains state across <code>process()</code> calls, enabling context-aware decisions on subsequent message batches. A complete runnable example is available at examples/basics/default_reasoner.py.</p>"},{"location":"basics/#concurrentgroupreasoner","title":"ConcurrentGroupReasoner","text":"<p><code>ConcurrentGroupReasoner</code> creates a separate reasoner agent for each user, each maintaining its own independent reasoning state. While all reasoner agents see the complete group chat context, each maintains a separate conversation history. Messages from different users can be processed concurrently, while messages from the same user are processed sequentially.</p> <pre><code>from group_sense import ConcurrentGroupReasoner, Decision, DefaultGroupReasonerFactory, Message\n\n\n# Load system prompt template with {owner} placeholder\ntemplate_path = Path(\"examples\", \"prompts\", \"concurrent\", \"fact_check.md\")\ntemplate = template_path.read_text()\n\n# Create factory and reasoner\nfactory = DefaultGroupReasonerFactory(system_prompt_template=template)\nreasoner = ConcurrentGroupReasoner(factory=factory)\n\n# Earlier messages establishing context\nlogger.info(\"Processing alice's message establishing context...\")\nawait reasoner.process(Message(content=\"The client meeting is on Thursday.\", sender=\"alice\"))\n\n# Process new messages from different users concurrently\nlogger.info(\"\\nProcessing messages from charlie and bob concurrently...\")\nf1 = reasoner.process(Message(content=\"Sounds good!\", sender=\"charlie\"))\nf2 = reasoner.process(Message(content=\"I'll prepare slides for the Friday meeting.\", sender=\"bob\"))\n\nresponse1 = await f1\nlogger.info(f\"Charlie - Decision: {response1.decision}\")\n\nresponse2 = await f2\nlogger.info(f\"Bob - Decision: {response2.decision}\")\nif response2.decision == Decision.DELEGATE:\n    logger.info(f\"Bob - Query: {response2.query}\")\n    logger.info(\"Bob's reasoner detects contradiction using full group context\")\n</code></pre> <p>Each user gets their own reasoner agent customized with their user ID via <code>DefaultGroupReasonerFactory</code>. A complete runnable example is available at examples/basics/concurrent_reasoner.py.</p>"},{"location":"examples/","title":"Examples","text":"<p>The following examples demonstrate different engagement patterns using simplified system prompts. For more elaborate prompts with detailed instructions, as used in Basics, see examples/prompts/.</p>"},{"location":"examples/#answer-assistance","title":"Answer Assistance","text":"<p>In group conversations, users sometimes cannot answer questions addressed to them. The reasoner detects this pattern and delegates the original question to the AI, setting the receiver to the original questioner. A complete runnable example is available at examples/example_1.py.</p> <pre><code>from group_sense import Decision, DefaultGroupReasoner, Message\n\n\n# Short prompt for delegation when users can't answer\nsystem_prompt = (\n    \"Delegate when a user indicates they can't answer a question \"\n    \"addressed to them. Transform to first-person query and set receiver to the question \"\n    \"sender. Ignore everything else.\"\n)\n\n# Create the reasoner with the system prompt\nreasoner = DefaultGroupReasoner(system_prompt=system_prompt)\n\n# Process alice's question to bob - should be ignored\nlogger.info(\"Processing alice's question to bob...\")\nresponse1 = await reasoner.process(\n    [\n        Message(\n            content=\"We need to add rate limiting to our API. Do you know how to implement that?\",\n            sender=\"alice\",\n            receiver=\"bob\",\n        ),\n    ]\n)\nlogger.info(f\"Decision: {response1.decision}\")\n\n# bob can't answer - should be delegated\nlogger.info(\"Processing bob's message...\")\nresponse2 = await reasoner.process(\n    [\n        Message(content=\"I'm not sure, let me check.\", sender=\"bob\"),\n    ]\n)\nlogger.info(f\"Decision: {response2.decision}\")\n\nif response2.decision == Decision.DELEGATE:\n    logger.info(f\"Query: {response2.query}\")\n    logger.info(f\"Respond to: {response2.receiver}\")\n    logger.info(\"The AI will research and respond to alice!\")\nelse:\n    logger.info(\"No delegation triggered\")\n</code></pre> <p>This example uses <code>DefaultGroupReasoner</code> to process messages and make delegation decisions based on the configured system prompt. The <code>Message</code> class represents individual chat messages, and <code>Decision</code> is an enum indicating whether to delegate or ignore.</p>"},{"location":"examples/#fact-checking","title":"Fact Checking","text":"<p>When participants provide conflicting information about facts, dates, or events, the reasoner detects the contradiction and generates verification queries without requiring explicit user requests. A complete runnable example is available at examples/example_2.py.</p> <pre><code>from group_sense import Decision, DefaultGroupReasoner, Message\n\n\n# Short custom prompt for fact-checking engagement\nsystem_prompt = (\n    \"Delegate when you detect contradictory information \"\n    \"about facts, dates, or events. Generate a query asking for verification. \"\n    \"Ignore everything else. Always set receiver to null.\"\n)\n\n# Create the reasoner with the fact-checking prompt\nreasoner = DefaultGroupReasoner(system_prompt=system_prompt)\n\n# Process messages with contradictory information\nlogger.info(\"Processing messages with contradictory meeting times...\")\nresponse = await reasoner.process(\n    [\n        Message(content=\"The meeting is tomorrow at 2pm.\", sender=\"alice\"),\n        Message(content=\"Thank you for the reminder, I'll be there.\", sender=\"charlie\"),\n        Message(content=\"I'll be there too, see you at 3pm.\", sender=\"bob\"),\n    ]\n)\n\n# Check the decision and display results\nlogger.info(f\"Decision: {response.decision}\")\n\nif response.decision == Decision.DELEGATE:\n    logger.info(f\"Query generated: {response.query}\")\n    logger.info(f\"Send to: {response.receiver}\")\n    logger.info(\"The reasoner detected a contradiction and wants verification!\")\nelse:\n    logger.info(\"No delegation - no contradiction detected\")\n</code></pre> <p>This example demonstrates how <code>DefaultGroupReasoner</code> can proactively identify patterns in group conversations and generate verification queries without explicit requests from users.</p>"},{"location":"examples/#general-assistance","title":"General Assistance","text":"<p>Provides assistance by handling direct questions and follow-up queries. Uses <code>ConcurrentGroupReasoner</code> so that reasoning runs for different users concurrently. This example is similar to direct assistant usage, but the reasoner handles all group context complexity: transforming group conversations into self-contained queries and managing per-user reasoning state. A complete runnable example is available at examples/example_3.py.</p> <pre><code>from group_sense import ConcurrentGroupReasoner, Decision, DefaultGroupReasonerFactory, Message\n\n\n# Template with {owner} placeholder for per-sender customization\ntemplate = (\n    \"You are assisting {owner} in a group chat. \"\n    \"Delegate when {owner} asks questions or continues conversations with the system. \"\n    \"Make delegate queries self-contained. Ignore everything else.\"\n)\n\nfactory = DefaultGroupReasonerFactory(system_prompt_template=template)\nreasoner = ConcurrentGroupReasoner(factory=factory)\n\n# Process alice's first message\n# Expected: DELEGATE with query \"What's the weather like in Vienna today?\"\nlogger.info(\"Processing alice's first message...\")\nf1 = reasoner.process(Message(content=\"What's the weather like in Vienna today?\", sender=\"alice\"))\nresponse1 = await f1\nlogger.info(f\"Alice message 1 - Decision: {response1.decision}\")\nif response1.decision == Decision.DELEGATE:\n    logger.info(f\"  Query: {response1.query}\")\n\n# Add AI response back to the shared context\nlogger.info(\"Adding system response to shared context...\")\nreasoner.append(Message(content=\"It's sunny in Vienna today.\", sender=\"system\"))\nlogger.info(\"System message added - available to all user contexts\")\n\n# Process bob and alice's messages concurrently\n# Bob expected: IGNORE (casual statement)\n# Alice expected: DELEGATE with self-contained query \"What's the weather like in Vienna tomorrow?\"\nlogger.info(\"\\nProcessing bob and alice's messages concurrently...\")\nf2 = reasoner.process(Message(content=\"I'm feeling good!\", sender=\"bob\"))\nf3 = reasoner.process(Message(content=\"and tomorrow?\", sender=\"alice\"))\n\nresponse2 = await f2\nlogger.info(f\"Bob message - Decision: {response2.decision}\")\nif response2.decision == Decision.DELEGATE:\n    logger.info(f\"  Query: {response2.query}\")\n\nresponse3 = await f3\nlogger.info(f\"Alice message 2 - Decision: {response3.decision}\")\nif response3.decision == Decision.DELEGATE:\n    logger.info(f\"  Query: {response3.query}\")\n    logger.info(\"  Notice: Follow-up to both alice's first message AND the system response!\")\n</code></pre> <p>This example uses <code>ConcurrentGroupReasoner</code> with <code>DefaultGroupReasonerFactory</code> to create per-user reasoning instances. Each user gets their own reasoner customized via the <code>{owner}</code> placeholder in the system prompt template.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#python-package","title":"Python Package","text":"<pre><code>pip install group-sense\n</code></pre>"},{"location":"installation/#development-setup","title":"Development Setup","text":"<p>For development setup and contributing guidelines, see DEVELOPMENT.md.</p>"},{"location":"installation/#api-keys","title":"API Keys","text":"<p>Group Sense uses Google Gemini models by default. Set your API key:</p> <pre><code>export GOOGLE_API_KEY=\"your-api-key\"\n</code></pre> <p>The library supports any pydantic-ai compatible model. Pass a custom model to reasoner constructors using the <code>model</code> parameter.</p>"},{"location":"integration/","title":"Integrating into Applications","text":"<p>The code example below demonstrates how to integrate Group Sense as an adapter between a group chat system and an existing single-user AI assistant. The pattern shows key integration steps: setting up the concurrent reasoner with custom prompts, handling incoming group messages, processing triage decisions, and feeding assistant responses back into the shared conversation context. A complete running implementation using Group Terminal as a group chat system is available at examples/chat/application.py.</p>"},{"location":"integration/#setup","title":"Setup","text":"<pre><code># Reasoner setup\ntemplate = self._load_reasoner_template(reasoner_template_name)\nself._factory = DefaultGroupReasonerFactory(system_prompt_template=template)\nself._reasoner = ConcurrentGroupReasoner(factory=self._factory)\n</code></pre>"},{"location":"integration/#message-handler","title":"Message Handler","text":"<pre><code>async def _handle_message(self, content: str, sender: str):\n    message = self._create_reasoner_message(content, sender)\n    # Initiate reasoner processing in message arrival order\n    # (guarantees equal internal and chat message ordering)\n    future = self._reasoner.process(message)\n    # Asynchronously await and process reasoner response\n    create_task(self._handle_response(future, sender))\n\nasync def _handle_response(self, future: Future[Response], sender: str):\n    try:\n        reasoner_response = await future\n    except Exception:\n        logger.exception(\"Reasoner error\")\n        return\n\n    logger.debug(f\"Reasoner decision: {reasoner_response.decision.value}\")\n    if reasoner_response.decision == Decision.IGNORE:\n        return\n\n    if not reasoner_response.query:\n        logger.warning(\"Reasoner delegated without query\")\n        return\n\n    try:\n        # Run downstream assistant with query generated by reasoner\n        logger.debug(f\"Assistant query: {reasoner_response.query}\")\n        assistant_response = await self._service.run(reasoner_response.query, sender=sender)\n    except Exception:\n        logger.exception(\"Assistant error\")\n    else:\n        logger.debug(f\"Assistant response: {assistant_response}\")\n\n        if reasoner_response.receiver:\n            # If reasoner set a dynamic receiver, @mention them in the chat message\n            assistant_response = f\"@{reasoner_response.receiver} {assistant_response}\"\n\n        message = Message(\n            content=assistant_response,\n            sender=\"system\",\n            receiver=reasoner_response.receiver,\n        )\n\n        # Add response message to reasoner\n        # (needed for concurrent reasoning)\n        self._reasoner.append(message)\n\n        # Send response to chat clients\n        await self._server.send_message(message.content, sender=message.sender)\n</code></pre>"},{"location":"api/message/","title":"Message","text":""},{"location":"api/message/#group_sense.Message","title":"group_sense.Message  <code>dataclass</code>","text":"<pre><code>Message(content: str, sender: str, receiver: str | None = None, threads: list[Thread] = list(), attachments: list[Attachment] = list())\n</code></pre> <p>A message in a group chat conversation.</p> <p>Represents a single message exchanged in a group chat environment. Messages can optionally target specific recipients, reference other threads, and include attachments.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The text content of the message.</p> <code>sender</code> <code>str</code> <p>User ID of the message sender.</p> <code>receiver</code> <code>str | None</code> <p>Optional user ID of the intended recipient. When set, indicates the message is directed at a specific user (e.g., via @mention).</p> <code>threads</code> <code>list[Thread]</code> <p>List of referenced threads from other group chats. Used for cross-conversation context.</p> <code>attachments</code> <code>list[Attachment]</code> <p>List of media or document attachments accompanying the message.</p>"},{"location":"api/message/#group_sense.Attachment","title":"group_sense.Attachment  <code>dataclass</code>","text":"<pre><code>Attachment(path: str, name: str, media_type: str)\n</code></pre> <p>Metadata for media or documents attached to group chat messages.</p> <p>Attachments allow messages to reference external media files or documents that accompany the text content.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>File path or URL to the attached resource.</p> <code>name</code> <code>str</code> <p>Display name of the attachment.</p> <code>media_type</code> <code>str</code> <p>MIME type of the attachment (e.g., 'image/png', 'application/pdf').</p>"},{"location":"api/message/#group_sense.Thread","title":"group_sense.Thread  <code>dataclass</code>","text":"<pre><code>Thread(id: str, messages: list[Message])\n</code></pre> <p>Reference to a group chat thread other than the current one.</p> <p>Threads allow messages to reference related discussions happening in other group chats, enabling cross-conversation context.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier of the referenced thread.</p> <code>messages</code> <code>list[Message]</code> <p>List of messages from the referenced thread.</p>"},{"location":"api/reasoner/","title":"Reasoner","text":""},{"location":"api/reasoner/#group_sense.Decision","title":"group_sense.Decision","text":"<p>               Bases: <code>Enum</code></p> <p>Decision outcome for message triage.</p> <p>Determines whether messages should be processed by the downstream application or ignored by the triage system.</p>"},{"location":"api/reasoner/#group_sense.Decision.DELEGATE","title":"DELEGATE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DELEGATE = 'delegate'\n</code></pre>"},{"location":"api/reasoner/#group_sense.Decision.IGNORE","title":"IGNORE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IGNORE = 'ignore'\n</code></pre>"},{"location":"api/reasoner/#group_sense.Response","title":"group_sense.Response  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Triage decision response for group chat messages.</p> <p>Encapsulates the triage decision and optional delegation parameters for processing messages from the group chat environment.</p> <p>Fields:</p> <ul> <li> <code>decision</code>                 (<code>Decision</code>)             </li> <li> <code>query</code>                 (<code>str | None</code>)             </li> <li> <code>receiver</code>                 (<code>str | None</code>)             </li> </ul>"},{"location":"api/reasoner/#group_sense.Response.decision","title":"decision  <code>pydantic-field</code>","text":"<pre><code>decision: Decision\n</code></pre>"},{"location":"api/reasoner/#group_sense.Response.query","title":"query  <code>pydantic-field</code>","text":"<pre><code>query: str | None = None\n</code></pre> <p>First-person query for the downstream application, formulated as if written by a single user. Required when decision is DELEGATE. Should be self-contained with all necessary context. Example: 'Can you help me understand how async/await works in Python?'</p>"},{"location":"api/reasoner/#group_sense.Response.receiver","title":"receiver  <code>pydantic-field</code>","text":"<pre><code>receiver: str | None = None\n</code></pre> <p>User ID of the intended recipient who should receive the downstream application's response. Required when decision is DELEGATE.</p>"},{"location":"api/reasoner/#group_sense.GroupReasoner","title":"group_sense.GroupReasoner","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract protocol for incremental group chat message processing.</p> <p>Defines the interface for reasoners that process group chat messages incrementally, maintaining conversation context across multiple calls. Each <code>process()</code> call represents a conversation turn that adds to the reasoner's history.</p> <p>Implementations decide whether message increments should be ignored or delegated to downstream AI systems for processing.</p>"},{"location":"api/reasoner/#group_sense.GroupReasoner.processed","title":"processed  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>processed: int\n</code></pre> <p>Number of messages processed so far by this reasoner.</p>"},{"location":"api/reasoner/#group_sense.GroupReasoner.process","title":"process  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>process(updates: list[Message]) -&gt; Response\n</code></pre> <p>Process a message increment and decide whether to delegate.</p> <p>Analyzes new messages in the context of the entire conversation history and decides whether to ignore them or generate a query for downstream AI processing.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>list[Message]</code> <p>List of new messages to process as an increment. Must not be empty. Represents messages that arrived since the last <code>process()</code> call.</p> required <p>Returns:</p> Type Description <code>Response</code> <p>Response containing the triage decision and optional delegation parameters (query and receiver).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If updates is empty.</p>"},{"location":"api/reasoner/#group_sense.GroupReasonerFactory","title":"group_sense.GroupReasonerFactory","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract factory protocol for creating GroupReasoner instances.</p> <p>Defines the interface for factories that create reasoner instances customized for specific owners. Used primarily by <code>ConcurrentGroupReasoner</code> to create per-sender reasoner instances.</p>"},{"location":"api/reasoner/#group_sense.GroupReasonerFactory.create_group_reasoner","title":"create_group_reasoner  <code>abstractmethod</code>","text":"<pre><code>create_group_reasoner(owner: str) -&gt; GroupReasoner\n</code></pre> <p>Create a new GroupReasoner instance for the specified owner.</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>str</code> <p>User ID of the reasoner instance owner. The reasoner will be customized for this user's perspective.</p> required <p>Returns:</p> Type Description <code>GroupReasoner</code> <p>A new GroupReasoner instance configured for the owner.</p>"},{"location":"api/reasoner/#group_sense.DefaultGroupReasoner","title":"group_sense.DefaultGroupReasoner","text":"<pre><code>DefaultGroupReasoner(system_prompt: str, model: str | Model | None = None, model_settings: ModelSettings | None = None)\n</code></pre> <p>               Bases: <code>GroupReasoner</code></p> <p>Sequential group chat message processor with single shared context.</p> <p>Processes group chat messages incrementally using a single reasoner agent that maintains conversation history across all <code>process()</code> calls. Suitable for scenarios where all messages are processed from a unified perspective without per-sender context separation.</p> <p>The reasoner uses an agent to decide whether each message increment should be ignored or delegated to downstream systems with a generated query.</p> Example <pre><code>reasoner = DefaultGroupReasoner(system_prompt=\"...\")\nresponse = await reasoner.process([message1, message2])\nif response.decision == Decision.DELEGATE:\n    print(f\"Query: {response.query}\")\n</code></pre> <p>Initialize the reasoner with a system prompt and optional model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>System prompt that defines the reasoner's behavior and decision-making criteria. Should not contain an {owner} placeholder.</p> required <code>model</code> <code>str | Model | None</code> <p>Optional AI model to use. Defaults to \"gemini-2.5-flash\". Can be a model name string or a pydantic-ai Model instance.</p> <code>None</code> <code>model_settings</code> <code>ModelSettings | None</code> <p>Optional model-specific settings. Defaults to GoogleModelSettings with thinking enabled.</p> <code>None</code>"},{"location":"api/reasoner/#group_sense.DefaultGroupReasoner.get_serialized","title":"get_serialized","text":"<pre><code>get_serialized() -&gt; dict[str, Any]\n</code></pre> <p>Serialize the reasoner's state for persistence.</p> <p>Captures the conversation history and message count for later restoration via <code>set_serialized()</code>. Used by applications to persist reasoner state across restarts or for debugging purposes.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing serialized conversation history and processed message count.</p>"},{"location":"api/reasoner/#group_sense.DefaultGroupReasoner.process","title":"process  <code>async</code>","text":"<pre><code>process(updates: list[Message]) -&gt; Response\n</code></pre> <p>Process a message increment and decide whether to delegate.</p> <p>Analyzes new messages in the context of the entire conversation history maintained by this reasoner. Each call adds to the conversation history, making subsequent calls aware of previous messages and decisions.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>list[Message]</code> <p>List of new messages to process as an increment. Must not be empty. Represents messages that arrived since the last <code>process()</code> call.</p> required <p>Returns:</p> Type Description <code>Response</code> <p>Response containing the triage decision (IGNORE or DELEGATE) and optional delegation parameters (query and receiver).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If updates is empty.</p>"},{"location":"api/reasoner/#group_sense.DefaultGroupReasoner.set_serialized","title":"set_serialized","text":"<pre><code>set_serialized(state: dict[str, Any])\n</code></pre> <p>Restore the reasoner's state from serialized data.</p> <p>Reconstructs the conversation history and message count from previously serialized state. Used by applications to restore reasoner state after restarts or for debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict[str, Any]</code> <p>Dictionary containing serialized state from <code>get_serialized()</code>. Must include 'agent' (conversation history) and 'processed' (message count) keys.</p> required"},{"location":"api/reasoner/#group_sense.DefaultGroupReasonerFactory","title":"group_sense.DefaultGroupReasonerFactory","text":"<pre><code>DefaultGroupReasonerFactory(system_prompt_template: str)\n</code></pre> <p>               Bases: <code>GroupReasonerFactory</code></p> <p>Factory for creating DefaultGroupReasoner instances with owner-specific prompts.</p> <p>Creates reasoner instances by substituting the {owner} placeholder in a system prompt template. Used primarily by <code>ConcurrentGroupReasoner</code> to create per-sender reasoner instances, where each sender gets their own reasoner customized with their user ID.</p> Example <pre><code>template = \"You are assisting {owner} in a group chat...\"\nfactory = DefaultGroupReasonerFactory(system_prompt_template=template)\nreasoner = factory.create_group_reasoner(owner=\"user123\")\n</code></pre> <p>Initialize the factory with a system prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt_template</code> <code>str</code> <p>Template string containing an {owner} placeholder that will be replaced with the actual owner ID when creating reasoner instances.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the template does not contain an {owner} placeholder.</p>"},{"location":"api/reasoner/#group_sense.DefaultGroupReasonerFactory.create_group_reasoner","title":"create_group_reasoner","text":"<pre><code>create_group_reasoner(owner: str, **kwargs: Any) -&gt; GroupReasoner\n</code></pre> <p>Create a DefaultGroupReasoner instance for the specified owner.</p> <p>Substitutes the {owner} placeholder in the template with the provided owner ID and creates a new reasoner instance.</p> <p>Parameters:</p> Name Type Description Default <code>owner</code> <code>str</code> <p>User ID to substitute into the {owner} placeholder.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to DefaultGroupReasoner constructor (e.g., model, model_settings).</p> <code>{}</code> <p>Returns:</p> Type Description <code>GroupReasoner</code> <p>A new DefaultGroupReasoner instance configured with the owner-specific system prompt.</p>"},{"location":"api/reasoner/#group_sense.ConcurrentGroupReasoner","title":"group_sense.ConcurrentGroupReasoner","text":"<pre><code>ConcurrentGroupReasoner(factory: GroupReasonerFactory)\n</code></pre> <p>Concurrent group chat processor with per-sender reasoner instances.</p> <p>Manages multiple reasoner instances (one per sender) that process messages concurrently. Maintains a shared list of all group chat messages that all reasoner instances can see, accessible via the messages property.</p> <p>Each sender gets their own reasoner instance with independent conversation context, but all instances see the same shared group chat messages. A reasoner instance is triggered only when its owner sends a message. Sequential execution per sender prevents concurrent state corruption to a single reasoner instance.</p> <p>The process() method returns a Future to allow callers to control message ordering: calling process() in the order messages arrive from the group chat ensures messages are stored internally in that same order.</p> Example <pre><code>factory = DefaultGroupReasonerFactory(system_prompt_template=\"...\")\nreasoner = ConcurrentGroupReasoner(factory=factory)\n\n# Process messages concurrently\nfuture1 = reasoner.process(Message(content=\"Hi\", sender=\"alice\"))\nfuture2 = reasoner.process(Message(content=\"Hello\", sender=\"bob\"))\n\n# Await responses\nresponse1 = await future1\nresponse2 = await future2\n\n# Add AI response to context without triggering reasoning\nreasoner.append(Message(content=\"How can I help?\", sender=\"system\"))\n</code></pre> <p>Initialize the concurrent reasoner with a factory.</p> <p>Parameters:</p> Name Type Description Default <code>factory</code> <code>GroupReasonerFactory</code> <p>Factory used to create per-sender reasoner instances. Each unique sender gets their own reasoner created via this factory.</p> required"},{"location":"api/reasoner/#group_sense.ConcurrentGroupReasoner.messages","title":"messages  <code>property</code>","text":"<pre><code>messages: list[Message]\n</code></pre> <p>The shared list of all group chat messages stored internally.</p>"},{"location":"api/reasoner/#group_sense.ConcurrentGroupReasoner.append","title":"append","text":"<pre><code>append(message: Message)\n</code></pre> <p>Add a message to the shared group chat context without triggering reasoning.</p> <p>Adds the message to the internally stored group chat message list that all reasoner instances share, without initiating a reasoning process. Typically used for AI-generated responses to prevent infinite reasoning loops while ensuring all reasoners see these messages.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to add to the shared group chat context. Typically messages with sender=\"system\" or other AI-generated content.</p> required"},{"location":"api/reasoner/#group_sense.ConcurrentGroupReasoner.process","title":"process","text":"<pre><code>process(message: Message) -&gt; Future[Response]\n</code></pre> <p>Process a message and return a Future for the reasoning result.</p> <p>Adds the message to the shared group chat message list and triggers the sender's reasoner instance. Returns a Future to allow the caller to control message ordering: calling <code>process()</code> in the order messages arrive from the group chat ensures they are stored internally in that same order.</p> <p>Processing happens asynchronously. Messages from different senders can be processed concurrently, while messages from the same sender are processed sequentially to prevent concurrent state corruption to that sender's reasoner instance.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>User message to process. The sender field determines which reasoner instance is triggered.</p> required <p>Returns:</p> Type Description <code>Future[Response]</code> <p>Future that will resolve to a Response containing the triage decision and optional delegation parameters. Use await or asyncio utilities to retrieve the result.</p> Example <pre><code># Store messages internally in arrival order, process concurrently\nf1 = reasoner.process(msg1)  # from alice\nf2 = reasoner.process(msg2)  # from bob\nf3 = reasoner.process(msg3)  # from alice\n\n# Messages stored internally as: msg1, msg2, msg3\n# Processing: msg1 and msg2 run concurrently, msg3 waits for msg1\n</code></pre>"}]}